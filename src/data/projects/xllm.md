---
title: xLLM
description: >-
  xLLM is an open-source framework for vision-language models, providing tools
  and documentation for training and inference.
date: 2025-11-06T17:52:05.992Z
oss_date: 2025-08-12T13:16:07.000Z
website: 'https://xllm.readthedocs.io/zh-cn/latest/'
github: 'https://github.com/jd-opensource/xllm'
author: jd-opensource
tags:
  - multimodal
  - training
  - inference
featured: false
thumbnail: 'https://opengraph.githubassets.com/1/jd-opensource/xllm'
category: Inference
---

## Detailed Introduction

xLLM is an open-source framework for vision-language models, offering training, fine-tuning, and inference tooling with documentation and examples to help research and engineering teams build multimodal systems.

## Main Features

- Supports joint training and inference pipelines for vision-language tasks.
- Provides multimodal data processing and evaluation tools.
- Comprehensive ReadTheDocs documentation and example code for engineering adoption.

## Use Cases

Suitable for research and product teams building visual question answering, image captioning, and multimodal retrieval systems.

## Technical Features

Focuses on multimodal feature fusion and cross-modal alignment, offering extensible model components and training strategies for large-scale training and fine-tuning.

---
title: MLC LLM
description: >-
  MLC LLM is a machine learning compiler and deployment engine that enables
  high-performance LLM inference across platforms using compilation and runtime
  optimizations.
date: 2025-09-27T04:43:45.460Z
website: 'https://llm.mlc.ai/'
github: 'https://github.com/mlc-ai/mlc-llm'
oss_date: 2023-04-29T01:59:25.000Z
author: MLC AI
tags:
  - inference
featured: false
thumbnail: 'https://opengraph.githubassets.com/1/mlc-ai/mlc-llm'
slug: mlc-llm
category: Inference
---

## Overview

MLC LLM is a compiler-driven deployment engine for large language models. It compiles and runs models efficiently on a wide range of platforms, including servers, browsers, and mobile devices.

## Key features

- Cross-platform backends (CUDA, Vulkan, Metal, WebGPU) and mobile support.
- Compiler optimizations that produce efficient model execution code and runtime scheduling.
- OpenAI-compatible APIs and SDKs for Python, JavaScript, and mobile platforms.

## Use cases

- Deploying LLM services across heterogeneous hardware to improve throughput and latency.
- Running LLMs in-browser or on mobile devices for low-latency edge applications.

## Technical notes

- MLCEngine unifies compilation and runtime, offering extensible backends and deployment tooling; follow the documentation at <https://llm.mlc.ai/docs/> for build and integration steps.

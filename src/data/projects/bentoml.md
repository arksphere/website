---
title: BentoML
slug: bentoml
date: 2025-12-11T14:41:11.659Z
website: https://bentoml.com
github: https://github.com/bentoml/bentoml
oss_date: 2019-04-02T01:39:27.000Z
author: BentoML
tags:
  - inference
description: BentoML is an open-source framework for packaging, containerizing, and deploying machine learning models into production-ready services.
featured: false
thumbnail: https://opengraph.githubassets.com/1/bentoml/BentoML
category: Inference
---

## Overview

BentoML helps engineers package any ML/AI model into a deployable service (a "Bento"), with one-click container image generation, model versioning, and multi-framework support to streamline development-to-production workflows.

## Key features

- Turn model inference code into REST/gRPC APIs with minimal boilerplate, supporting async and batched calls.
- Build reproducible deployment artifacts (Bento) and generate Docker images for simplified release processes.
- Support adaptive batching, model parallelism, and multi-model orchestration to improve throughput and resource utilization.
- Rich examples, plugins and integrations for runtime extensions and cloud deployment.

## Use cases

- Online inference APIs and microservice-based model deployments.
- Multi-model inference pipelines and task-queue-driven workloads.
- Quickly move research models into cloud or edge production environments.

## Technical highlights

- Python-native developer experience; compatible with PyTorch, TensorFlow, Transformers and other major frameworks.
- Model Store and version management; pluggable runtime optimizations such as adaptive batching.
- Integrations with container tooling, CI/CD pipelines, and cloud platforms (e.g., BentoCloud) for production-grade deployments.

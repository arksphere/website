---
title: LMDeploy
slug: lmdeploy
date: 2025-12-11T14:39:46.836Z
website: https://lmdeploy.readthedocs.io/en/latest/
github: https://github.com/internlm/lmdeploy
oss_date: 2023-06-15T12:38:06.000Z
author: InternLM
tags:
  - inference
description: LMDeploy is a toolkit for compressing, deploying and serving large language models, providing optimized inference engines, quantization and distribution features.
featured: false
thumbnail: https://opengraph.githubassets.com/1/InternLM/lmdeploy
category: Inference
---

## Overview

LMDeploy provides end-to-end model compression, quantization and deployment capabilities, including high-performance engines (TurboMind), continuous batching and distribution services for latency-sensitive production workloads.

## Key features

- High-performance inference engines (TurboMind and optimized PyTorch backends).
- Quantization and KV-cache optimization to reduce memory footprint and latency.
- Deployment and distribution for offline batch and online multi-host serving.

## Use cases

- Convert research models into production inference services with minimal effort.
- Serve high-concurrency, low-latency applications such as chat APIs.

## Technical notes

- Supports multiple backends and model formats; see project docs for compatible models and installation.
- Includes benchmarking and visualization tooling for performance evaluation.
